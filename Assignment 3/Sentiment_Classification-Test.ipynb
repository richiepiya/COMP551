{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 : Datasets can be found in the dataGenerated folder\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def getFeatures(setName, path):\n",
    "    textSections = []\n",
    "    \n",
    "    # Read file\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        for l in f.readlines():\n",
    "            splitText = l.split('\\t') # Split into sections\n",
    "            splitText = [x.strip() for x in splitText] # Get rid of whitespace\n",
    "            textSections.append(splitText)\n",
    "    # Join text sections to make string\n",
    "    text = ','.join(map(str, textSections))\n",
    "    # Remove punctuation\n",
    "    temp = str.maketrans(\" \", \" \", string.punctuation)\n",
    "    processedText = text.translate(temp)\n",
    "    # To lowercase\n",
    "    processedText = processedText.lower()\n",
    "    # Build list with occurrence count\n",
    "    occMap = Counter(processedText.split()).most_common()\n",
    "    \n",
    "    # Get top 10,000 features\n",
    "    topFeatures = []\n",
    "    for i in range(numFeatures):\n",
    "        topFeatures.append(occMap[i])\n",
    "\n",
    "    # Recompute weights by reversing order\n",
    "    dictionary = []\n",
    "    for i in range(numFeatures-1, -1, -1):\n",
    "        dictionary.append([topFeatures[i][0], i+1])\n",
    "    \n",
    "    # Write vocabulary\n",
    "    # Format: \"word,  ID (reversed order),  occurrence\"\n",
    "    writer = open(\"dataGenerated/\" + setName + \"vocab.txt\", \"w\")\n",
    "    for i in range(numFeatures):\n",
    "        line = \"{}\\t{}\\t{}\".format(topFeatures[i][0], i+1, topFeatures[i][1])\n",
    "        writer.write(line + \"\\n\")\n",
    "        \n",
    "    # Write train, valid, test sets\n",
    "    # Format: \"ID (of text),  classLabel\"\n",
    "    for d in datasets:\n",
    "        writer = open(\"dataGenerated/\" + setName + d + \".txt\", \"w\")\n",
    "        # Read file\n",
    "        with open(\"data/\" + setName + d + \".txt\", 'r', encoding=\"utf-8\") as f:\n",
    "            for l in f.readlines():\n",
    "                splitText = l.split('\\t') # Split into sections\n",
    "                splitText = [x.strip() for x in splitText] # Get rid of whitespace\n",
    "                textSections.append(splitText) \n",
    "\n",
    "        # Keep copy of text sections (used to build map of original text)\n",
    "        tempText = textSections\n",
    "        textList = []\n",
    "        temp = str.maketrans(\" \", \" \", string.punctuation)\n",
    "        for i in range(len(tempText)):\n",
    "            # Remove punctuation and lowercase\n",
    "            processedSection = str(tempText[i]).translate(temp)\n",
    "            processedSection = processedSection.lower()\n",
    "            textList.append(processedSection)\n",
    "            \n",
    "        # Iterate through each section of text\n",
    "        for i in range(len(textList)):\n",
    "            words = textList[i].split()\n",
    "            classLabel = words[len(words)-1]\n",
    "            line = \"\"\n",
    "            \n",
    "            for j in range(len(words)):\n",
    "                for x in dictionary:\n",
    "                    if (x[0] == words[j]):\n",
    "                        line += str(x[1]) + \" \"\n",
    "                    \n",
    "            line += \"\\t\" + classLabel + \"\\n\"\n",
    "            writer.write(line)\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def getBOW(setName, features):\n",
    "    binaryBOW = []\n",
    "    freqBOW = []\n",
    "    \n",
    "    for d in datasets:\n",
    "        path = \"data/\" + setName + d + \".txt\"\n",
    "        textSections = []\n",
    "        \n",
    "        # Read file and create string\n",
    "        with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "            for l in f.readlines():\n",
    "                splitText = l.split('\\t') # Split into sections\n",
    "                splitText = [x.strip() for x in splitText] # Get rid of whitespace\n",
    "                textSections.append(splitText)\n",
    "        text = ','.join(map(str, textSections)) # Join to make string\n",
    "\n",
    "        # Remove punctuation\n",
    "        temp = str.maketrans(\" \", \" \", string.punctuation)\n",
    "        processedText = text.translate(temp)\n",
    "        # To lowercase\n",
    "        processedText = processedText.lower()\n",
    "        \n",
    "        # Perform binary and frequency bag of words\n",
    "        # Binary BOW indicates if word appears\n",
    "        # Freq BOW indicates the number of times it appears\n",
    "        setBinaryBOW = []\n",
    "        setFreqBOW = []\n",
    "        for i in range(numFeatures):\n",
    "            # Check if text contains the word\n",
    "            if (features[i][0] in processedText):\n",
    "                setBinaryBOW.append(1)\n",
    "            else:\n",
    "                setBinaryBOW.append(0)\n",
    "            # Count how many times the text contains the word\n",
    "            setFreqBOW.append(processedText.count(features[i][0]))\n",
    "    \n",
    "        binaryBOW.append(setBinaryBOW)\n",
    "        freqBOW.append(setFreqBOW)\n",
    "    \n",
    "    return binaryBOW, freqBOW\n",
    "    \n",
    "    \n",
    "numFeatures = 10000\n",
    "datasets = [\"train\", \"valid\", \"test\"]\n",
    "yelpTrainPath = \"data/yelp-train.txt\"\n",
    "IMDBTrainPath = \"data/IMDB-train.txt\"\n",
    "\n",
    "features = getFeatures(\"yelp-\", yelpTrainPath)\n",
    "yelpBinary, yelpFreq = getBOW(\"yelp-\", features)\n",
    "features = getFeatures(\"IMDB-\", IMDBTrainPath)\n",
    "IMDBBinary, IMDBFreq = getBOW(\"IMDB-\", features)\n",
    "\n",
    "print(\"Part 1 : Datasets can be found in the dataGenerated folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables stored\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"dataTemp/yelpBinary.txt\", \"wb\") as fp:\n",
    "    pickle.dump(yelpBinary, fp)\n",
    "    \n",
    "with open(\"dataTemp/yelpFreq.txt\", \"wb\") as fp:\n",
    "    pickle.dump(yelpFreq, fp)\n",
    "    \n",
    "with open(\"dataTemp/IMDBBinary.txt\", \"wb\") as fp:\n",
    "    pickle.dump(IMDBBinary, fp)\n",
    "    \n",
    "with open(\"dataTemp/IMDBFreq.txt\", \"wb\") as fp:\n",
    "    pickle.dump(IMDBFreq, fp)\n",
    "    \n",
    "print(\"All variables stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables loaded\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"dataTemp/yelpBinary.txt\", \"rb\") as fp:\n",
    "    yelpBinary = pickle.load(fp)\n",
    "    \n",
    "with open(\"dataTemp/yelpFreq.txt\", \"rb\") as fp:\n",
    "    yelpFreq = pickle.load(fp)\n",
    "    \n",
    "with open(\"dataTemp/IMDBBinary.txt\", \"rb\") as fp:\n",
    "    IMDBBinary = pickle.load(fp)\n",
    "    \n",
    "with open(\"dataTemp/IMDBFreq.txt\", \"rb\") as fp:\n",
    "    IMDBFreq = pickle.load(fp)\n",
    "\n",
    "print(\"All variables loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Classifier: \n",
      "Train, Valid, Test: (1.0, 0.74756113858078321, 0.80126613704071503)\n",
      "\n",
      "Majority Classifier: \n",
      "Train, Valid, Test: (1.0, 0.85642403796672195, 0.92397912519502878)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "target = []\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "        target.append(count)\n",
    "        count+=1\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "target = np.array(target)\n",
    "\n",
    "trainData = np.array(yelpBinary[0])\n",
    "trainData_matrix = sparse.csr_matrix((trainData, (row,col)))\n",
    "trainData_matrix = trainData_matrix.toarray()\n",
    "\n",
    "validData = np.array(yelpBinary[1])\n",
    "validData_matrix = sparse.csr_matrix((validData, (row,col)))\n",
    "validData_matrix = validData_matrix.toarray()\n",
    "\n",
    "testData = np.array(yelpBinary[2])\n",
    "testData_matrix = sparse.csr_matrix((testData, (row,col)))\n",
    "testData_matrix = testData_matrix.toarray()\n",
    "\n",
    "\n",
    "def trainModel(dataset, clf):\n",
    "    clf = clf.fit(trainData_matrix[:-1], target[:-1])\n",
    "\n",
    "    pred_train = []\n",
    "    pred_valid = []\n",
    "    pred_test = []\n",
    "    for i in range(10000):\n",
    "        # Predict last values, gets index\n",
    "        temp_train = clf.predict(trainData_matrix[-i:])\n",
    "        temp_valid = clf.predict(trainData_matrix[-i:])\n",
    "        temp_test = clf.predict(trainData_matrix[-i:])\n",
    "        # Get predicted element from original dataset\n",
    "        pred_train.append(trainData[temp_train[len(temp_train)-1]])\n",
    "        pred_valid.append(validData[temp_valid[len(temp_valid)-1]])\n",
    "        pred_test.append(validData[temp_test[len(temp_test)-1]])\n",
    "    pred_train = np.array(pred_train)\n",
    "    pred_valid = np.array(pred_valid)\n",
    "    pred_test = np.array(pred_test)\n",
    "    \n",
    "    predictionTrain = f1_score(dataset[0], pred_train, average=\"binary\")\n",
    "    predictionValid = f1_score(dataset[1], pred_valid, average=\"binary\")\n",
    "    predictionTest = f1_score(dataset[2], pred_test, average=\"binary\")\n",
    "\n",
    "    return predictionTrain, predictionValid, predictionTest\n",
    "        \n",
    "\n",
    "\n",
    "# Random Classifier\n",
    "f1_measure = trainModel(yelpBinary, DummyClassifier(strategy=\"uniform\"))\n",
    "print(\"\\nRandom Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))\n",
    "\n",
    "# Majority Classifier\n",
    "f1_measure = trainModel(yelpBinary, DummyClassifier(strategy=\"most_frequent\"))\n",
    "print(\"Majority Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
