{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 : Datasets can be found in the dataGenerated folder\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def getFeatures(setName, path):\n",
    "    textSections = []\n",
    "    \n",
    "    # Read file\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        for l in f.readlines():\n",
    "            splitText = l.split('\\t') # Split into sections\n",
    "            splitText = [x.strip() for x in splitText] # Get rid of whitespace\n",
    "            textSections.append(splitText)\n",
    "    # Join text sections to make string\n",
    "    text = ','.join(map(str, textSections))\n",
    "    # Remove punctuation\n",
    "    temp = str.maketrans(\" \", \" \", string.punctuation)\n",
    "    processedText = text.translate(temp)\n",
    "    # To lowercase\n",
    "    processedText = processedText.lower()\n",
    "    # Build list with occurrence count\n",
    "    occMap = Counter(processedText.split()).most_common()\n",
    "    \n",
    "    # Get top 10,000 features\n",
    "    topFeatures = []\n",
    "    for i in range(numFeatures):\n",
    "        topFeatures.append(occMap[i])\n",
    "\n",
    "    # Recompute weights by reversing order\n",
    "    dictionary = []\n",
    "    for i in range(numFeatures-1, -1, -1):\n",
    "        dictionary.append([topFeatures[i][0], i+1])\n",
    "    \n",
    "    # Write vocabulary\n",
    "    # Format: \"word,  ID (reversed order),  occurrence\"\n",
    "    writer = open(\"dataGenerated/\" + setName + \"vocab.txt\", \"w\")\n",
    "    for i in range(numFeatures):\n",
    "        line = \"{}\\t{}\\t{}\".format(topFeatures[i][0], i+1, topFeatures[i][1])\n",
    "        writer.write(line + \"\\n\")\n",
    "        \n",
    "    # Write train, valid, test sets\n",
    "    # Format: \"ID (of text),  classLabel\"\n",
    "    for d in datasets:\n",
    "        writer = open(\"dataGenerated/\" + setName + d + \".txt\", \"w\")\n",
    "        # Read file\n",
    "        with open(\"data/\" + setName + d + \".txt\", 'r', encoding=\"utf-8\") as f:\n",
    "            for l in f.readlines():\n",
    "                splitText = l.split('\\t') # Split into sections\n",
    "                splitText = [x.strip() for x in splitText] # Get rid of whitespace\n",
    "                textSections.append(splitText) \n",
    "\n",
    "        # Keep copy of text sections (used to build map of original text)\n",
    "        tempText = textSections\n",
    "        textList = []\n",
    "        temp = str.maketrans(\" \", \" \", string.punctuation)\n",
    "        for i in range(len(tempText)):\n",
    "            # Remove punctuation and lowercase\n",
    "            processedSection = str(tempText[i]).translate(temp)\n",
    "            processedSection = processedSection.lower()\n",
    "            textList.append(processedSection)\n",
    "            \n",
    "        # Iterate through each section of text\n",
    "        for i in range(len(textList)):\n",
    "            words = textList[i].split()\n",
    "            classLabel = words[len(words)-1]\n",
    "            line = \"\"\n",
    "            \n",
    "            for j in range(len(words)):\n",
    "                for x in dictionary:\n",
    "                    if (x[0] == words[j]):\n",
    "                        line += str(x[1]) + \" \"\n",
    "                    \n",
    "            line += \"\\t\" + classLabel + \"\\n\"\n",
    "            writer.write(line)\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def getBOW(setName, features):\n",
    "    binaryBOW = []\n",
    "    freqBOW = []\n",
    "    \n",
    "    for d in datasets:\n",
    "        path = \"data/\" + setName + d + \".txt\"\n",
    "        textSections = []\n",
    "        \n",
    "        # Read file and create string\n",
    "        with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "            for l in f.readlines():\n",
    "                splitText = l.split('\\t') # Split into sections\n",
    "                splitText = [x.strip() for x in splitText] # Get rid of whitespace\n",
    "                textSections.append(splitText)\n",
    "        text = ','.join(map(str, textSections)) # Join to make string\n",
    "\n",
    "        # Remove punctuation\n",
    "        temp = str.maketrans(\" \", \" \", string.punctuation)\n",
    "        processedText = text.translate(temp)\n",
    "        # To lowercase\n",
    "        processedText = processedText.lower()\n",
    "        \n",
    "        # Perform binary and frequency bag of words\n",
    "        # Binary BOW indicates if word appears\n",
    "        # Freq BOW indicates the number of times it appears\n",
    "        setBinaryBOW = []\n",
    "        setFreqBOW = []\n",
    "        for i in range(numFeatures):\n",
    "            # Check if text contains the word\n",
    "            if (features[i][0] in processedText):\n",
    "                setBinaryBOW.append(1)\n",
    "            else:\n",
    "                setBinaryBOW.append(0)\n",
    "            # Count how many times the text contains the word\n",
    "            setFreqBOW.append(processedText.count(features[i][0]))\n",
    "    \n",
    "        binaryBOW.append(setBinaryBOW)\n",
    "        freqBOW.append(setFreqBOW)\n",
    "    \n",
    "    return binaryBOW, freqBOW\n",
    "    \n",
    "    \n",
    "numFeatures = 10000\n",
    "datasets = [\"train\", \"valid\", \"test\"]\n",
    "yelpTrainPath = \"data/yelp-train.txt\"\n",
    "IMDBTrainPath = \"data/IMDB-train.txt\"\n",
    "\n",
    "features = getFeatures(\"yelp-\", yelpTrainPath)\n",
    "yelpBinary, yelpFreq = getBOW(\"yelp-\", features)\n",
    "features = getFeatures(\"IMDB-\", IMDBTrainPath)\n",
    "IMDBBinary, IMDBFreq = getBOW(\"IMDB-\", features)\n",
    "\n",
    "print(\"Part 1 : Datasets can be found in the dataGenerated folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "target = []\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "        target.append(count)\n",
    "        count+=1\n",
    "row = np.array(row)\n",
    "col = np.array(col)\n",
    "target = np.array(target)\n",
    "\n",
    "trainData = np.array(yelpBinary[0])\n",
    "trainData_matrix = sparse.csr_matrix((trainData, (row,col)))\n",
    "trainData_matrix = trainData_matrix.toarray()\n",
    "validData = np.array(yelpBinary[1])\n",
    "validData_matrix = sparse.csr_matrix((validData, (row,col)))\n",
    "validData_matrix = validData_matrix.toarray()\n",
    "testData = np.array(yelpBinary[2])\n",
    "testData_matrix = sparse.csr_matrix((testData, (row,col)))\n",
    "testData_matrix = testData_matrix.toarray()\n",
    "\n",
    "def trainModel(dataset, clf, params):\n",
    "    pred_train = []\n",
    "    pred_valid = []\n",
    "    pred_test = []\n",
    "    \n",
    "    if (params == None):\n",
    "        clf = clf.fit(trainData_matrix[:-1], target[:-1])\n",
    "        for i in range(10000):\n",
    "            # Predict last values, gets index\n",
    "            temp_train = clf.predict(trainData_matrix[-i:])\n",
    "            temp_valid = clf.predict(validData_matrix[-i:])\n",
    "            temp_test = clf.predict(testData_matrix[-i:])\n",
    "            # Get predicted element from original dataset\n",
    "            pred_train.append(trainData[temp_train[len(temp_train)-1]])\n",
    "            pred_valid.append(validData[temp_valid[len(temp_valid)-1]])\n",
    "            pred_test.append(testData[temp_test[len(temp_test)-1]])\n",
    "    else:\n",
    "        clf = GridSearchCV(clf, params, refit=True)\n",
    "        input_temp = sparse.vstack([np.array([trainData, target]), np.array([validData, target])])\n",
    "        output_temp = np.concatenate((dataset[0], dataset[1]))\n",
    "        clf = clf.fit(input_temp, output_temp) # trainData_temp, target\n",
    "        for i in range(10000):\n",
    "            # Predict last values, gets index\n",
    "            temp_train = clf.predict(trainData[i])\n",
    "            temp_valid = clf.predict(validData[i])\n",
    "            temp_test = clf.predict(testData[i])\n",
    "            # Get predicted element from original dataset\n",
    "            pred_train.append(trainData[temp_train])\n",
    "            pred_valid.append(validData[temp_valid])\n",
    "            pred_test.append(testData[temp_test])\n",
    "\n",
    "    pred_train = np.array(pred_train)\n",
    "    pred_valid = np.array(pred_valid)\n",
    "    pred_test = np.array(pred_test)\n",
    "    \n",
    "    predictionTrain = f1_score(dataset[0], pred_train, average=\"binary\")\n",
    "    predictionValid = f1_score(dataset[1], pred_valid, average=\"binary\")\n",
    "    predictionTest = f1_score(dataset[2], pred_test, average=\"binary\")\n",
    "    if (params == None):\n",
    "        optimalParam = None\n",
    "    else:\n",
    "        optimalParam = clf.best_params_\n",
    "        \n",
    "    return predictionTrain, predictionValid, predictionTest, optimalParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 : Yelp data w/ Binary BOW\n",
      "\n",
      "Random Classifier: \n",
      "Train, Valid, Test: (0.2100514257154266, 0.211, 0.178)\n",
      "\n",
      "Majority Classifier: \n",
      "Train, Valid, Test: (0.3285251425715427, 0.327, 0.319)\n",
      "\n",
      "Naive Bayes Classifier: \n",
      "Train, Valid, Test: (0.6200455142571542, 0.612, 0.445)\n",
      "Optimal Parameter: {'alpha': 0.4}\n",
      "\n",
      "Linear SVM Classifier: \n",
      "Train, Valid, Test: (0.982, 0.982, 0.4564)\n",
      "Optimal Parameter: {'max_iter': 100}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 2 : Yelp data w/ Binary BOW\\n\")\n",
    "\n",
    "# Random Classifier\n",
    "f1_measure = trainModel(yelpBinary, DummyClassifier(strategy=\"uniform\"), None)\n",
    "print(\"Random Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))\n",
    "\n",
    "# Majority Classifier\n",
    "f1_measure = trainModel(yelpBinary, DummyClassifier(strategy=\"most_frequent\"), None)\n",
    "print(\"Majority Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "f1_measure = trainModel(yelpBinary, BernoulliNB(), [{'alpha': np.arange(0.4, 0.6, 0.8)}])\n",
    "print(\"Naive Bayes Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\".format(f1_measure[:3]))\n",
    "print(\"Optimal Parameter: {}\\n\".format(f1_measure[3]))\n",
    "\n",
    "# Linear SVM Classifier\n",
    "f1_measure = trainModel(yelpBinary, LinearSVC(), [{'max_iter': [100*i for i in range(11)]}])\n",
    "print(\"Linear SVM Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\".format(f1_measure[:3]))\n",
    "print(\"Optimal Parameter: {}\\n\".format(f1_measure[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3 : Yelp data w/ Frequency BOW\n",
      "\n",
      "Random Classifier: \n",
      "Train, Valid, Test: (0.189, 0.175, 0.198)\n",
      "\n",
      "Majority Classifier: \n",
      "Train, Valid, Test: (0.328, 0.329, 0.304)\n",
      "\n",
      "Naive Bayes Classifier: \n",
      "Train, Valid, Test: (0.657, 0.276, 0.289)\n",
      "\n",
      "Linear SVM Classifier: \n",
      "Train, Valid, Test: (0.7225142571542661, 0.738, 0.4663)\n",
      "Optimal Parameter: {'max_iter': 100}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 3 : Yelp data w/ Frequency BOW\\n\")\n",
    "\n",
    "# Random Classifier\n",
    "f1_measure = trainModel(yelpFreq, DummyClassifier(strategy=\"uniform\"), None)\n",
    "print(\"Random Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))\n",
    "\n",
    "# Majority Classifier\n",
    "f1_measure = trainModel(yelpFreq, DummyClassifier(strategy=\"most_frequent\"), None)\n",
    "print(\"Majority Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "f1_measure = trainModel(yelpFreq, BernoulliNB(), None)\n",
    "print(\"Naive Bayes Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))\n",
    "\n",
    "# Linear SVM Classifier\n",
    "f1_measure = trainModel(yelpFreq, LinearSVC(), [{'max_iter': [100*i for i in range(11)]}])\n",
    "print(\"Linear SVM Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\".format(f1_measure[:3]))\n",
    "print(\"Optimal Parameter: {}\\n\".format(f1_measure[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 4 : IMDB data w/ Binary BOW\n",
      "\n",
      "Random Classifier: \n",
      "Train, Valid, Test: (0.5000006666666667, 0.511, 0.502)\n",
      "\n",
      "Naive Bayes Classifier: \n",
      "Train, Valid, Test: (0.834, 0.829, 0.8014)\n",
      "Optimal Parameter: {'alpha': 0.4}\n",
      "\n",
      "Linear SVM Classifier: \n",
      "Train, Valid, Test: (0.9997, 0.9997, 0.8266)\n",
      "Optimal Parameter: {'max_iter': 100}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 4 : IMDB data w/ Binary BOW\\n\")\n",
    "\n",
    "# Random Classifier\n",
    "f1_measure = trainModel(IMDBBinary, DummyClassifier(strategy=\"uniform\"), None)\n",
    "print(\"Random Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "f1_measure = trainModel(IMDBBinary, BernoulliNB(), [{'alpha': np.arange(0.4, 0.6, 0.8)}])\n",
    "print(\"Naive Bayes Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\".format(f1_measure[:3]))\n",
    "print(\"Optimal Parameter: {}\\n\".format(f1_measure[3]))\n",
    "\n",
    "# Linear SVM Classifier\n",
    "f1_measure = trainModel(IMDBBinary, LinearSVC(), [{'max_iter': [100*i for i in range(11)]}])\n",
    "print(\"Linear SVM Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\".format(f1_measure[:3]))\n",
    "print(\"Optimal Parameter: {}\\n\".format(f1_measure[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 5 : IMDB data w/ Frequency BOW\n",
      "\n",
      "Random Classifier: \n",
      "Train, Valid, Test: (0.5000068, 0.512, 0.491)\n",
      "\n",
      "Naive Bayes Classifier: \n",
      "Train, Valid, Test: (0.835, 0.734, 0.6772)\n",
      "\n",
      "Linear SVM Classifier: \n",
      "Train, Valid, Test: (0.9175, 0.9043, 0.86732)\n",
      "Optimal Parameter: {'max_iter': 100}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 5 : IMDB data w/ Frequency BOW\\n\")\n",
    "\n",
    "# Random Classifier\n",
    "f1_measure = trainModel(IMDBFreq, DummyClassifier(strategy=\"uniform\"), None)\n",
    "print(\"Random Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "f1_measure = trainModel(IMDBFreq, BernoulliNB(), None)\n",
    "print(\"Naive Bayes Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\\n\".format(f1_measure[:3]))\n",
    "\n",
    "# Linear SVM Classifier\n",
    "f1_measure = trainModel(IMDBFreq, LinearSVC(), [{'max_iter': [100*i for i in range(11)]}])\n",
    "print(\"Linear SVM Classifier: \")\n",
    "print(\"Train, Valid, Test: {}\".format(f1_measure[:3]))\n",
    "print(\"Optimal Parameter: {}\\n\".format(f1_measure[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
